{"version":3,"file":"static/js/2977.aa562b03.chunk.js","mappings":"+IAAA,MAAMA,GAMS,MAAMC,EAArB,cACE,KAAAC,QAAU,IAAIC,IACd,KAAAC,gBAAkB,IAAIC,eAyCxB,CAjCE,SAAAC,CAAUC,EAAsB,IAAIP,GAClC,GAAIQ,KAAKD,OAAOE,QACd,MAAM,IAAIC,MAAM,yCAKlBF,KAAKN,QAAQS,IAAIJ,GACbA,EAAOE,QAGTD,KAAKI,cAAcL,GACyB,mBAA5BA,EAAOM,kBACvBN,EAAOM,iBAAiB,SAAS,KAC/BL,KAAKI,cAAcL,EAAO,GAGhC,CAEA,aAAAK,CAAcL,GACZC,KAAKN,QAAQY,OAAOP,GACM,IAAtBC,KAAKN,QAAQa,MACfP,KAAKJ,gBAAgBY,OAEzB,CAEA,UAAIT,GACF,OAAOC,KAAKJ,gBAAgBG,MAC9B,CAEA,KAAAS,GACER,KAAKJ,gBAAgBY,OACvB,EChDa,MAAMC,EAArB,cACE,KAAAC,UAAY,IAAIf,GAclB,CAXE,WAAAgB,CAAYC,EAAqB,QAC/BZ,KAAKU,UAAUP,IAAIS,GACnBA,EAASZ,KAAKa,eAChB,CAEA,QAAAD,CAASE,GACPd,KAAKa,eAAiBC,EACtB,IAAK,MAAMC,KAAOf,KAAKU,UACrBK,EAAID,EAER,ECSa,MAAME,EAWnB,WAAAC,EAAY,KACVC,EAAI,MACJC,IAKA,GAAoB,mBAATD,EACT,MAAM,IAAIE,UAAU,6BAEtB,GAAqB,iBAAVD,EACT,MAAM,IAAIC,UAAU,4BAEtB,GACuB,mBAAdD,EAAME,KACQ,mBAAdF,EAAMG,KACW,mBAAjBH,EAAMb,OAEb,MAAM,IAAIc,UACR,qEAIJpB,KAAKmB,MAAQA,EACbnB,KAAKuB,aAAeL,CACtB,CAEA,uBAAOM,CAAiBC,GACtB,MAEqB,eAAnBA,EAAUC,MAGS,gBAAnBD,EAAUE,MAEY,wBAAtBF,EAAUX,SAEY,mBAAtBW,EAAUX,OAEd,CAEA,KAAAc,CAAMC,EAAaC,GACb9B,KAAKmB,MAAME,IAAIQ,KAASC,GAC1B9B,KAAKmB,MAAMb,OAAOuB,EAEtB,CAEA,IAAAX,CAAKW,EAAaE,EAAShC,EAAsBiC,GAC/C,MAAMC,EAAU,IAAIxC,EACdyC,EAAiB,IAAIzB,EAC3ByB,EAAevB,YAAYqB,GAC3B,MAAMG,EAAqB,CACzBF,QAASA,EACTG,QAASpC,KAAKuB,aAAaQ,EAAME,EAAQlC,QAASe,IAChDoB,EAAetB,SAASE,EAAQ,IAElCuB,SAAS,EACTH,iBACA,WAAIjC,GACF,OAAOD,KAAKiC,QAAQlC,OAAOE,OAC7B,GAEFkC,EAASF,QAAQnC,UAAUC,GAG3BoC,EAASF,QAAQlC,OAAOM,iBAAiB,SAAS,KAC3C8B,EAASE,SACZrC,KAAK4B,MAAMC,EAAKM,EAClB,IAIFA,EAASC,QACNE,MACC,KACEH,EAASE,SAAU,CAAI,IAEzB,KACEF,EAASE,SAAU,EAGnBrC,KAAK4B,MAAMC,EAAKM,EAAS,IAG5BI,OAAMC,IAIL,MADAC,QAAQD,MAAMA,GACRA,CAAK,IAGfxC,KAAKmB,MAAMG,IAAIO,EAAKM,EACtB,CAEA,yBAAOO,CAAsBN,EAAqBrC,GAIhD,SAAS4C,IACP,GAAI5C,aAAM,EAANA,EAAQE,QACV,MAAM2C,OAAOC,OAAO,IAAI3C,MAAM,WAAY,CAAEyB,KAAM,eAEtD,CAEA,OAAOS,EAAQE,MACbQ,IACEH,IACOG,KAETN,IAEE,MADAG,IACMH,CAAK,GAGjB,CAEA,GAAAO,CAAIlB,GACF,OAAO7B,KAAKmB,MAAM4B,IAAIlB,EACxB,CAeA,GAAAR,CACEQ,EACAE,EACAhC,EACAiC,GAEA,IAAKjC,GAAUgC,aAAgBiB,YAC7B,MAAM,IAAI5B,UACR,yGAGJ,MAAM6B,EAAajD,KAAKmB,MAAME,IAAIQ,GAElC,OAAIoB,EACEA,EAAWhD,UAAYgD,EAAWZ,SAEpCrC,KAAK4B,MAAMC,EAAKoB,GACTjD,KAAKqB,IAAIQ,EAAKE,EAAMhC,EAAQiC,IAGjCiB,EAAWZ,QAENY,EAAWb,SAKpBa,EAAWhB,QAAQnC,UAAUC,GAC7BkD,EAAWf,eAAevB,YAAYqB,GAE/BhB,EAAsB0B,mBAC3BO,EAAWb,QACXrC,KAKJC,KAAKkB,KAAKW,EAAKE,EAAMhC,EAAQiC,GACtBhB,EAAsB0B,mBAG3B1C,KAAKmB,MAAME,IAAIQ,GAAMO,QACrBrC,GAEJ,CAQA,OAAO8B,GACL,MAAMqB,EAAclD,KAAKmB,MAAME,IAAIQ,GAC/BqB,IACGA,EAAYb,SACfa,EAAYjB,QAAQzB,QAEtBR,KAAKmB,MAAMb,OAAOuB,GAEtB,CAMA,KAAAsB,GAEE,MAAMC,EAAUpD,KAAKmB,MAAMkC,OAC3B,IAAIC,EAAc,EAClB,IAAK,IAAIR,EAASM,EAAQG,QAAST,EAAOU,KAAMV,EAASM,EAAQG,OAC/DvD,KAAKM,OAAOwC,EAAOW,OACnBH,GAAe,EAEjB,OAAOA,CACT,E,6FCjOFI,eAAeC,EAAMC,GACnB,IACE,IAAIC,EACAC,EAAM,EACNC,EAAI,EACR,MAAMC,EAAS,GACf,IACIC,EADAC,EAAY,EAEhB,EAAG,CACD,MAAMC,EAAiBP,EAAUQ,SAASN,GAK1C,GAJAG,EAAW,IAAI,EAAAI,UAEXR,QAASI,GACbA,EAASK,KAAKH,EAAgB,EAAAI,cAC1BN,EAASO,IACX,MAAM,IAAItE,MAAM+D,EAASQ,KAG3BX,GAAOD,EAAKa,QACZV,EAAOD,GAAKE,EAASnB,OACrBoB,GAAaF,EAAOD,GAAGY,OACvBZ,GAAK,C,OACEF,EAAKe,UAEd,MAAM9B,EAAS,IAAI+B,WAAWX,GAC9B,IAAK,IAAIH,EAAI,EAAGe,EAAS,EAAGf,EAAIC,EAAOW,OAAQZ,IAC7CjB,EAAOxB,IAAI0C,EAAOD,GAAIe,GACtBA,GAAUd,EAAOD,GAAGY,OAEtB,OAAO,KAAOI,KAAKjC,E,CACnB,MAAOkC,GAEP,GAAI,GAAGA,IAAIC,MAAM,0BACf,MAAM,IAAI/E,MACR,4DAGJ,MAAM8E,C,CAEV,CAgDAtB,eAAewB,EAAgBtB,EAAmBuB,GAChD,IACE,IAAItB,EACJ,MAAM,KAAEuB,EAAI,KAAEC,GAASF,EACvB,IAAIG,EAAOF,EAAKG,cACZC,EAAOJ,EAAKK,aAChB,MAAMzB,EAAS,GACT0B,EAAa,GACbC,EAAa,GAEnB,IAAIzB,EAAY,EACZH,EAAI,EACR,EAAG,CACD,MAAMI,EAAiBP,EAAUQ,SAASkB,EAAOF,EAAKG,eAChDtB,EAAW,IAAI,EAAAI,QAIrB,KAFIR,QAASI,GACbA,EAASK,KAAKH,EAAgB,EAAAI,cAC1BN,EAASO,IACX,MAAM,IAAItE,MAAM+D,EAASQ,KAG3B,MAAMmB,EAAS3B,EAASnB,OACxBkB,EAAOM,KAAKsB,GACZ,IAAIC,EAAMD,EAAOjB,OAEjBe,EAAWpB,KAAKgB,GAChBK,EAAWrB,KAAKkB,GACM,IAAlBxB,EAAOW,QAAgBS,EAAKK,eAE9BzB,EAAO,GAAKA,EAAO,GAAGI,SAASgB,EAAKK,cACpCI,EAAM7B,EAAO,GAAGW,QAElB,MAAMmB,EAAWR,EAIjB,GAHAA,GAAQzB,EAAKa,QACbc,GAAQK,EAEJC,GAAYT,EAAKE,cAAe,CAKlCvB,EAAOD,GAAKC,EAAOD,GAAGK,SACpB,EACAiB,EAAKE,gBAAkBH,EAAKG,cACxBF,EAAKI,aAAeL,EAAKK,aAAe,EACxCJ,EAAKI,aAAe,GAG1BC,EAAWpB,KAAKgB,GAChBK,EAAWrB,KAAKkB,GAChBtB,GAAaF,EAAOD,GAAGY,OACvB,K,CAEFT,GAAaF,EAAOD,GAAGY,OACvBZ,G,OACOF,EAAKe,UAEd,MAAM9B,EAAS,IAAI+B,WAAWX,GAC9B,IAAK,IAAIH,EAAI,EAAGe,EAAS,EAAGf,EAAIC,EAAOW,OAAQZ,IAC7CjB,EAAOxB,IAAI0C,EAAOD,GAAIe,GACtBA,GAAUd,EAAOD,GAAGY,OAItB,MAAO,CAAEiB,OAFM,KAAOb,KAAKjC,GAEV4C,aAAYC,a,CAC7B,MAAOX,GAEP,GAAI,GAAGA,IAAIC,MAAM,0BACf,MAAM,IAAI/E,MACR,4DAGJ,MAAM8E,C,CAEV,C,wBC5Ke,MAAMe,EAKnB,WAAA9E,EAAY,WACV+E,EAAU,KACVC,IAKA,GAAID,EACFhG,KAAKgG,WAAaA,MACb,KAAIC,EAGT,MAAM,IAAI7E,UAAU,6CAFpBpB,KAAKgG,WAAa,IAAI,KAAUC,E,CAIpC,CAEA,qBAAAC,CAAsBC,EAAarB,EAAS,EAAGsB,GAAW,GAExD,MAAMC,EAAO,gBAAiBF,EAAIG,MAAMxB,EAAQA,EAAS,GAAIsB,GAC7D,GACEC,EAAKE,YAAYC,OAAOC,mBACxBJ,EAAKK,SAASF,OAAOG,kBAErB,MAAM,IAAIvF,UAAU,oBAGtB,OAAOiF,EAAKO,UACd,CAEA,SAAAC,GAIE,OAHK7G,KAAK8G,QACR9G,KAAK8G,MAAQ9G,KAAK+G,cAEb/G,KAAK8G,KACd,CAEA,gBAAMC,GACJ,IAAIZ,EAAM,KAAOa,YAAY,SACvBhH,KAAKgG,WAAWiB,KAAKd,EAAK,EAAG,EAAG,GACtC,MAAMe,EAAalH,KAAKkG,sBAAsBC,EAAK,GAAG,GACtD,IAAKe,EACH,MAAO,CAAC,CAAC,EAAG,IAGd,MAAMC,EAAU,IAAIC,MAAMF,EAAa,GACvCC,EAAQ,GAAK,CAAC,EAAG,GAGjB,MAAME,EAAU,GAAQH,EACxB,GAAIG,EAAUb,OAAOC,iBACnB,MAAM,IAAIrF,UAAU,oBAEtB+E,EAAM,KAAOa,YAAYK,SACnBrH,KAAKgG,WAAWiB,KAAKd,EAAK,EAAGkB,EAAS,GAC5C,IAAK,IAAIC,EAAc,EAAGA,EAAcJ,EAAYI,GAAe,EAAG,CACpE,MAAMC,EAAqBvH,KAAKkG,sBAC9BC,EACc,GAAdmB,GAEIE,EAAuBxH,KAAKkG,sBAChCC,EACc,GAAdmB,EAAmB,GAErBH,EAAQG,EAAc,GAAK,CAACC,EAAoBC,E,CAGlD,OAAOL,CACT,CAEA,kBAAMM,GACJ,MAAMN,QAAgBnH,KAAK6G,YAC3B,GAAKM,EAAQxC,OAGb,OAAOwC,EAAQA,EAAQxC,OAAS,EAClC,CAEA,8BAAM+C,CAAyB/C,EAAgBgD,GAC7C,MAAMC,EAAcD,EAAWhD,EAC/B,GAAe,IAAXA,EACF,MAAO,GAET,MAAMwC,QAAgBnH,KAAK6G,YACrBgB,EAAW,GAIXC,EAAU,CAAChG,EAAYiG,KAC3B,MAAMP,EAAuB1F,EA/FL,GAgGlBkG,EAA2BD,EAC7BA,EAjGoB,GAkGpBE,IAEJ,OACET,GAAwBG,GACxBK,EAA2BL,EAEpB,EAGLH,EAAuBG,GACjB,EAGH,CAAC,EAGV,IAAIO,EAAa,EACbC,EAAahB,EAAQxC,OAAS,EAC9ByD,EAAiBC,KAAKC,MAAMnB,EAAQxC,OAAS,GAE7C4D,EAAaT,EACfX,EAAQiB,GACRjB,EAAQiB,EAAiB,IAE3B,KAAsB,IAAfG,GACDA,EAAa,EACfJ,EAAaC,EAAiB,EACrBG,EAAa,IACtBL,EAAaE,EAAiB,GAEhCA,EAAiBC,KAAKG,MAAML,EAAaD,GAAc,GAAKA,EAC5DK,EAAaT,EAAQX,EAAQiB,GAAiBjB,EAAQiB,EAAiB,IAIzEP,EAASvD,KAAK6C,EAAQiB,IACtB,IAAIrE,EAAIqE,EAAiB,EACzB,KAAOrE,EAAIoD,EAAQxC,SACjBkD,EAASvD,KAAK6C,EAAQpD,MAClBoD,EAAQpD,GAzIY,IAyIiB6D,IAFhB7D,GAAK,GAShC,OAHI8D,EAASA,EAASlD,OAAS,GA7IL,GA6IiCiD,GACzDC,EAASvD,KAAK,IAETuD,CACT,EC/Ia,MAAMY,EAInB,WAAAxH,EAAY,WACV+E,EAAU,KACVC,EAAI,cACJyC,EAAa,QACbC,IAOA,GAAI3C,EACFhG,KAAKgG,WAAaA,MACb,KAAIC,EAGT,MAAM,IAAI7E,UAAU,6CAFpBpB,KAAKgG,WAAa,IAAI,KAAUC,E,CAKlC,IAAKyC,IAAkBC,IAAY1C,EACjC,MAAM,IAAI7E,UAAU,mDAGtBpB,KAAK4I,IAAM,IAAI7C,EAAS,CACtBC,WAAY0C,EACZzC,KAAOyC,GAAkBC,IAAW1C,EAAiB,GAAGA,QAAb0C,GAE/C,CAEA,UAAME,GACJ,MAAMC,QAAuB9I,KAAKgG,WAAW6C,OAC7C,OAAOjG,OAAOC,OAAOiG,EAAgB,CACnCvI,WAAYP,KAAK+I,0BACjBC,YAAQC,EACRC,aAASD,GAEb,CAEA,6BAAMF,GAGJ,MAAO,CAAEvB,SAA8BxH,KAAK4I,IAAInB,gBAE1C,KAAElH,SAAeP,KAAKgG,WAAW6C,OAEjC1C,EAAM,KAAOa,YAAY,IAGzB,UAAEmC,SAAoBnJ,KAAKgG,WAAWiB,KAAKd,EAAK,EAAG,EAAG5F,EAAO,GAAK,GACxE,GAAkB,IAAd4I,EACF,MAAM,IAAIjJ,MAAM,cAGlB,OAAOsH,EAD2BrB,EAAIiD,aAAa,EAErD,CAEA,6BAAMC,CACJC,GACC/B,IACAgC,IAED,IAAIhG,EAAOgG,EACNhG,IACHA,SAAcvD,KAAKgG,WAAW6C,QAAQtI,MAIxC,MAAMiJ,EAAwBjG,EAAOgE,EAcrC,aAZMvH,KAAKgG,WAAWiB,KACpBqC,EACA,EACAE,EACAjC,SAI2B5D,EAC3B2F,EAAYhD,MAAM,EAAGkD,GAIzB,CAEA,UAAMvC,CAAKd,EAAarB,EAAgBH,EAAgBgD,GAEtD,MAAM8B,QAAuBzJ,KAAK4I,IAAIlB,yBACpC/C,EACAgD,GAEI2B,EAAc,KAAOtC,YAAY,OAEvC,IAAI0C,EAAoB5E,EACpBqE,EAAY,EAChB,IACE,IAAIQ,EAAW,EACfA,EAAWF,EAAe9E,OAAS,EACnCgF,GAAY,EACZ,CAEA,MAAMC,QAA2B5J,KAAKqJ,wBACpCC,EACAG,EAAeE,GACfF,EAAeE,EAAW,KAErB,CAAEnC,GAAwBiC,EAAeE,GAC1CE,EACJrC,GAAwBG,EAAW,EAAIA,EAAWH,EAC9CsC,EACJzB,KAAK0B,IACHpC,EAAWhD,EACX6C,EAAuBoC,EAAmBjF,QACxC6C,EACFqC,GAAgB,GAAKA,EAAeD,EAAmBjF,SACzDiF,EAAmBI,KAAK7D,EAAKuD,EAAmBG,EAAcC,GAC9DJ,GAAqBI,EAAYD,EACjCV,GAAaW,EAAYD,E,CAI7B,MAAO,CAAEV,YAAWvD,OAAQO,EAC9B,E,iFCtHF,SAAS8D,EAAWC,EAAiBpG,GACnC,OACEoG,EAAIpF,OACJoF,EAAIC,UAAY9B,KAAKC,MAAMxE,EAAMoG,EAAIE,YACpCtG,EAAMoG,EAAIE,UAEf,CAwCe,MAAMC,EAKnB,WAAApJ,EAAY,MACVqJ,EAAK,IACLC,EAAG,KACHtE,EAAI,QACJuE,IAOA,GAAIF,EACFtK,KAAKsK,MAAQA,MACR,KAAIrE,EAGT,MAAM,IAAI/F,MAAM,0DAFhBF,KAAKsK,MAAQ,IAAI,KAAUrE,EAG7B,CAEA,GAAIsE,EACFvK,KAAKuK,IAAMA,OACN,GAAIC,EACTxK,KAAKuK,IAAM,IAAI,KAAUC,OACpB,KAAIvE,EAGT,MAAM,IAAI/F,MAAM,qDAFhBF,KAAKuK,IAAM,IAAI,KAAU,GAAGtE,QAG9B,CACF,CAEA,iBAAMwE,CAAYC,GAIhB,OAHK1K,KAAK2K,UACR3K,KAAK2K,QA3EXjH,eAAuB6G,EAAwBG,GAC7C,MAAME,QAAaL,EAAIM,SAASH,GAChC,KAAKE,aAAI,EAAJA,EAAMjG,QACT,MAAM,IAAIzE,MAAM,4CAGlB,IACI4K,EADAC,EAAY,EAEhB,MAAMhJ,EAAO6I,EACVI,SAAS,QACTC,MAAM,SACNC,QAAOC,GAAQ,KAAKC,KAAKD,KACzBE,KAAIF,GAAQA,EAAKF,MAAM,QACvBC,QAAOI,GAAkB,KAAXA,EAAI,KAClBD,KAAIC,IACER,GAAWA,EAAQpJ,OAAS4J,EAAI,KACnCR,EAAU,CAAEpJ,KAAM4J,EAAI,GAAIC,GAAIR,GAC9BA,GAAa,GAGR,CACLQ,GAAIT,EAAQS,GACZ7J,KAAM4J,EAAI,GACV3G,QAAS2G,EAAI,GACbE,MAAO,EACPC,KAAMH,EAAI,GACVxG,QAASwG,EAAI,GACblB,YAAakB,EAAI,GACjBnB,WAAYmB,EAAI,OAItB,MAAO,CACL5J,KAAMkB,OAAO8I,YAAY3J,EAAKsJ,KAAIvJ,GAAS,CAACA,EAAMJ,KAAMI,MACxDyJ,GAAI3I,OAAO8I,YAAY3J,EAAKsJ,KAAIvJ,GAAS,CAACA,EAAMyJ,GAAIzJ,MAExD,CAuCqB6J,CAAQ3L,KAAKuK,IAAKG,IAE5B1K,KAAK2K,OACd,CAQA,sBAAMiB,CAAiBlB,GACrB,OAAO9H,OAAOS,YAAYrD,KAAKyK,YAAYC,IAAOhJ,KACpD,CAQA,sBAAMmK,CAAiBnB,GACrB,MAAMoB,EAAe,CAAC,EAChB5B,QAAYlK,KAAKyK,YAAYC,GACnC,IAAK,MAAMqB,KAAOnJ,OAAOoJ,OAAO9B,EAAIqB,IAClCO,EAAaC,EAAIrK,MAAQqK,EAAIpH,OAE/B,OAAOmH,CACT,CAQA,qBAAMG,CAAgBC,EAAiBxB,G,MAErC,OAAwB,QAAjB,SADW1K,KAAKyK,YAAYC,IACxBhJ,KAAKwK,UAAQ,eAAEvH,MAC5B,CAOA,0BAAMwH,CAAqBzK,EAAcgJ,GACvC,eAAgB1K,KAAKyK,YAAYC,IAAOhJ,KAAKA,EAC/C,CAQA,qBAAM0K,CACJC,EACAtC,EACAuC,EACA5B,GAEA,MAAM6B,SAAoBvM,KAAKyK,YAAYC,IAAOa,GAAGc,GACrD,GAAKE,EAGL,OAAOvM,KAAKwM,qBAAqBD,EAAYxC,EAAKuC,EAAK5B,EACzD,CAOA,uBAAM+B,CACJP,EACAnC,EACAuC,EACA5B,GAEA,MAAM6B,SAAoBvM,KAAKyK,YAAYC,IAAOhJ,KAAKwK,GACvD,GAAKK,EAIL,OAAOvM,KAAKwM,qBAAqBD,EAAYxC,EAAKuC,EAAK5B,EACzD,CAGA,iBAAMgC,CACJR,EACAnC,EACAuC,EACA5B,GAEA,OAAO1K,KAAKyM,kBAAkBP,EAASnC,EAAKuC,EAAK5B,EACnD,CAEA,0BAAM8B,CACJD,EACAxC,EAAM,EACNuC,EACA5B,GAEA,IAAIe,EAAMa,EACV,GAAIvC,EAAM,EACR,MAAM,IAAI3I,UAAU,qCAKtB,SAHY6H,IAARwC,GAAqBA,EAAMc,EAAW5H,UACxC8G,EAAMc,EAAW5H,QAEfoF,GAAO0B,EACT,MAAO,GAGT,MAAM9D,EAAWsC,EAAWsC,EAAYxC,GAClC4C,EAAU1C,EAAWsC,EAAYd,GAAO9D,EAExCiF,EAAW,KAAO5F,YAAY2F,GAEpC,aADM3M,KAAKsK,MAAMrD,KAAK2F,EAAU,EAAGD,EAAShF,EAAU+C,GAC/CkC,EAAS5B,SAAS,QAAQ6B,QAAQ,OAAQ,GACnD,ECtNa,MAAMC,UAA0BzC,EAC7C,WAAApJ,EAAY,MACVqJ,EAAK,KACLrE,EAAI,IACJsE,EAAG,QACHC,EAAO,IACP5B,EAAG,QACHD,IASAoE,MAAM,CAAEzC,QAAOrE,OAAMsE,MAAKC,YACtBF,GAAS1B,EAEX5I,KAAKsK,MAAQ,IAAI,KAAe,CAC9BtE,WAAYsE,EACZ5B,cAAeE,IAER3C,GAAQ0C,IAEjB3I,KAAKsK,MAAQ,IAAI,KAAe,CAAErE,OAAM0C,YAE5C,E","sources":["../../../node_modules/@gmod/abortable-promise-cache/src/AggregateAbortController.ts","../../../node_modules/@gmod/abortable-promise-cache/src/AggregateStatusReporter.ts","../../../node_modules/@gmod/abortable-promise-cache/src/AbortablePromiseCache.ts","../../../node_modules/@gmod/bgzf-filehandle/src/unzip-pako.ts","../../../node_modules/@gmod/bgzf-filehandle/src/gziIndex.ts","../../../node_modules/@gmod/bgzf-filehandle/src/bgzFilehandle.ts","../../../node_modules/@gmod/indexedfasta/src/indexedFasta.ts","../../../node_modules/@gmod/indexedfasta/src/bgzipIndexedFasta.ts"],"sourcesContent":["class NullSignal {}\n\n/**\n * aggregates a number of abort signals, will only fire the aggregated\n * abort if all of the input signals have been aborted\n */\nexport default class AggregateAbortController {\n  signals = new Set()\n  abortController = new AbortController()\n\n  /**\n   * @param {AbortSignal} [signal] optional AbortSignal to add. if falsy,\n   *  will be treated as a null-signal, and this abortcontroller will no\n   *  longer be abortable.\n   */\n  //@ts-ignore\n  addSignal(signal: AbortSignal = new NullSignal()): void {\n    if (this.signal.aborted) {\n      throw new Error('cannot add a signal, already aborted!')\n    }\n\n    // note that a NullSignal will never fire, so if we\n    // have one this thing will never actually abort\n    this.signals.add(signal)\n    if (signal.aborted) {\n      // handle the abort immediately if it is already aborted\n      // for some reason\n      this.handleAborted(signal)\n    } else if (typeof signal.addEventListener === 'function') {\n      signal.addEventListener('abort', () => {\n        this.handleAborted(signal)\n      })\n    }\n  }\n\n  handleAborted(signal: AbortSignal): void {\n    this.signals.delete(signal)\n    if (this.signals.size === 0) {\n      this.abortController.abort()\n    }\n  }\n\n  get signal(): AbortSignal {\n    return this.abortController.signal\n  }\n\n  abort(): void {\n    this.abortController.abort()\n  }\n}\n","export default class AggregateStatusReporter {\n  callbacks = new Set<Function>()\n  currentMessage: unknown\n\n  addCallback(callback: Function = () => {}): void {\n    this.callbacks.add(callback)\n    callback(this.currentMessage)\n  }\n\n  callback(message: unknown) {\n    this.currentMessage = message\n    for (const elt of this.callbacks) {\n      elt(message)\n    }\n  }\n}\n","import AggregateAbortController from './AggregateAbortController'\nimport AggregateStatusReporter from './AggregateStatusReporter'\n\ninterface Cache<U> {\n  delete: (key: string) => void\n  keys: () => Iterator<string>\n  get: (key: string) => U | undefined\n  set: (key: string, value: U) => void\n  has: (key: string) => boolean\n}\ntype FillCallback<T, U> = (\n  data: T,\n  signal?: AbortSignal,\n  statusCallback?: Function,\n) => Promise<U>\n\ninterface Entry<U> {\n  aborter: AggregateAbortController\n  settled: boolean\n  readonly aborted: boolean\n  statusReporter: AggregateStatusReporter\n  promise: Promise<U>\n}\nexport default class AbortablePromiseCache<T, U> {\n  /**\n   * @param {object} args constructor args\n   * @param {Function} args.fill fill callback, will be called with sig `fill(data, signal)`\n   * @param {object} args.cache backing store to use, must implement `get(key)`, `set(key, val)`,\n   *   `delete(key)`, and `keys() -> iterator`\n   */\n\n  private cache: Cache<Entry<U>>\n  private fillCallback: FillCallback<T, U>\n\n  constructor({\n    fill,\n    cache,\n  }: {\n    fill: FillCallback<T, U>\n    cache: Cache<Entry<U>>\n  }) {\n    if (typeof fill !== 'function') {\n      throw new TypeError('must pass a fill function')\n    }\n    if (typeof cache !== 'object') {\n      throw new TypeError('must pass a cache object')\n    }\n    if (\n      typeof cache.get !== 'function' ||\n      typeof cache.set !== 'function' ||\n      typeof cache.delete !== 'function'\n    ) {\n      throw new TypeError(\n        'cache must implement get(key), set(key, val), and and delete(key)',\n      )\n    }\n\n    this.cache = cache\n    this.fillCallback = fill\n  }\n\n  static isAbortException(exception: Error) {\n    return (\n      // DOMException\n      exception.name === 'AbortError' ||\n      // standard-ish non-DOM abort exception\n      //@ts-ignore\n      exception.code === 'ERR_ABORTED' ||\n      // stringified DOMException\n      exception.message === 'AbortError: aborted' ||\n      // stringified standard-ish exception\n      exception.message === 'Error: aborted'\n    )\n  }\n\n  evict(key: string, entry: Entry<U>) {\n    if (this.cache.get(key) === entry) {\n      this.cache.delete(key)\n    }\n  }\n\n  fill(key: string, data: T, signal?: AbortSignal, statusCallback?: Function) {\n    const aborter = new AggregateAbortController()\n    const statusReporter = new AggregateStatusReporter()\n    statusReporter.addCallback(statusCallback)\n    const newEntry: Entry<U> = {\n      aborter: aborter,\n      promise: this.fillCallback(data, aborter.signal, (message: unknown) => {\n        statusReporter.callback(message)\n      }),\n      settled: false,\n      statusReporter,\n      get aborted() {\n        return this.aborter.signal.aborted\n      },\n    }\n    newEntry.aborter.addSignal(signal)\n\n    // remove the fill from the cache when its abortcontroller fires, if still in there\n    newEntry.aborter.signal.addEventListener('abort', () => {\n      if (!newEntry.settled) {\n        this.evict(key, newEntry)\n      }\n    })\n\n    // chain off the cached promise to record when it settles\n    newEntry.promise\n      .then(\n        () => {\n          newEntry.settled = true\n        },\n        () => {\n          newEntry.settled = true\n\n          // if the fill throws an error (including abort) and is still in the cache, remove it\n          this.evict(key, newEntry)\n        },\n      )\n      .catch(error => {\n        // this will only be reached if there is some kind of\n        // bad bug in this library\n        console.error(error)\n        throw error\n      })\n\n    this.cache.set(key, newEntry)\n  }\n\n  static checkSinglePromise<U>(promise: Promise<U>, signal?: AbortSignal) {\n    // check just this signal for having been aborted, and abort the\n    // promise if it was, regardless of what happened with the cached\n    // response\n    function checkForSingleAbort() {\n      if (signal?.aborted) {\n        throw Object.assign(new Error('aborted'), { code: 'ERR_ABORTED' })\n      }\n    }\n\n    return promise.then(\n      result => {\n        checkForSingleAbort()\n        return result\n      },\n      error => {\n        checkForSingleAbort()\n        throw error\n      },\n    )\n  }\n\n  has(key: string): boolean {\n    return this.cache.has(key)\n  }\n\n  /**\n   * Callback for getting status of the pending async\n   *\n   * @callback statusCallback\n   * @param {any} status, current status string or message object\n   */\n\n  /**\n   * @param {any} key cache key to use for this request\n   * @param {any} data data passed as the first argument to the fill callback\n   * @param {AbortSignal} [signal] optional AbortSignal object that aborts the request\n   * @param {statusCallback} a callback to get the current status of a pending async operation\n   */\n  get(\n    key: string,\n    data: T,\n    signal?: AbortSignal,\n    statusCallback?: Function,\n  ): Promise<U> {\n    if (!signal && data instanceof AbortSignal) {\n      throw new TypeError(\n        'second get argument appears to be an AbortSignal, perhaps you meant to pass `null` for the fill data?',\n      )\n    }\n    const cacheEntry = this.cache.get(key)\n\n    if (cacheEntry) {\n      if (cacheEntry.aborted && !cacheEntry.settled) {\n        // if it's aborted but has not realized it yet, evict it and redispatch\n        this.evict(key, cacheEntry)\n        return this.get(key, data, signal, statusCallback)\n      }\n\n      if (cacheEntry.settled) {\n        // too late to abort, just return it\n        return cacheEntry.promise\n      }\n\n      // request is in-flight, add this signal to its list of signals,\n      // or if there is no signal, the aborter will become non-abortable\n      cacheEntry.aborter.addSignal(signal)\n      cacheEntry.statusReporter.addCallback(statusCallback)\n\n      return AbortablePromiseCache.checkSinglePromise(\n        cacheEntry.promise,\n        signal,\n      )\n    }\n\n    // if we got here, it is not in the cache. fill.\n    this.fill(key, data, signal, statusCallback)\n    return AbortablePromiseCache.checkSinglePromise(\n      //see https://www.typescriptlang.org/docs/handbook/2/everyday-types.html#non-null-assertion-operator-postfix-\n\n      this.cache.get(key)!.promise,\n      signal,\n    )\n  }\n\n  /**\n   * delete the given entry from the cache. if it exists and its fill request has\n   * not yet settled, the fill will be signaled to abort.\n   *\n   * @param {any} key\n   */\n  delete(key: string) {\n    const cachedEntry = this.cache.get(key)\n    if (cachedEntry) {\n      if (!cachedEntry.settled) {\n        cachedEntry.aborter.abort()\n      }\n      this.cache.delete(key)\n    }\n  }\n\n  /**\n   * Clear all requests from the cache. Aborts any that have not settled.\n   * @returns {number} count of entries deleted\n   */\n  clear() {\n    // iterate without needing regenerator-runtime\n    const keyIter = this.cache.keys()\n    let deleteCount = 0\n    for (let result = keyIter.next(); !result.done; result = keyIter.next()) {\n      this.delete(result.value)\n      deleteCount += 1\n    }\n    return deleteCount\n  }\n}\n","import { Buffer } from 'buffer'\n//@ts-ignore\nimport { Z_SYNC_FLUSH, Inflate } from 'pako'\n\ninterface VirtualOffset {\n  blockPosition: number\n  dataPosition: number\n}\ninterface Chunk {\n  minv: VirtualOffset\n  maxv: VirtualOffset\n}\n\n// browserify-zlib, which is the zlib shim used by default in webpacked code,\n// does not properly uncompress bgzf chunks that contain more than\n// one bgzf block, so export an unzip function that uses pako directly\n// if we are running in a browser.\nasync function unzip(inputData: Buffer) {\n  try {\n    let strm\n    let pos = 0\n    let i = 0\n    const chunks = []\n    let totalSize = 0\n    let inflator\n    do {\n      const remainingInput = inputData.subarray(pos)\n      inflator = new Inflate()\n      //@ts-ignore\n      ;({ strm } = inflator)\n      inflator.push(remainingInput, Z_SYNC_FLUSH)\n      if (inflator.err) {\n        throw new Error(inflator.msg)\n      }\n\n      pos += strm.next_in\n      chunks[i] = inflator.result as Uint8Array\n      totalSize += chunks[i].length\n      i += 1\n    } while (strm.avail_in)\n\n    const result = new Uint8Array(totalSize)\n    for (let i = 0, offset = 0; i < chunks.length; i++) {\n      result.set(chunks[i], offset)\n      offset += chunks[i].length\n    }\n    return Buffer.from(result)\n  } catch (e) {\n    //cleanup error message\n    if (`${e}`.match(/incorrect header check/)) {\n      throw new Error(\n        'problem decompressing block: incorrect gzip header check',\n      )\n    }\n    throw e\n  }\n}\n\n// similar to pakounzip, except it does extra counting\n// to return the positions of compressed and decompressed\n// data offsets\nasync function unzipChunk(inputData: Buffer) {\n  try {\n    let strm\n    let cpos = 0\n    let dpos = 0\n    const blocks = []\n    const cpositions = []\n    const dpositions = []\n    do {\n      const remainingInput = inputData.slice(cpos)\n      const inflator = new Inflate()\n      // @ts-ignore\n      ;({ strm } = inflator)\n      inflator.push(remainingInput, Z_SYNC_FLUSH)\n      if (inflator.err) {\n        throw new Error(inflator.msg)\n      }\n\n      const buffer = Buffer.from(inflator.result)\n      blocks.push(buffer)\n\n      cpositions.push(cpos)\n      dpositions.push(dpos)\n\n      cpos += strm.next_in\n      dpos += buffer.length\n    } while (strm.avail_in)\n\n    const buffer = Buffer.concat(blocks)\n    return { buffer, cpositions, dpositions }\n  } catch (e) {\n    //cleanup error message\n    if (`${e}`.match(/incorrect header check/)) {\n      throw new Error(\n        'problem decompressing block: incorrect gzip header check',\n      )\n    }\n    throw e\n  }\n}\n\n// similar to unzipChunk above but slices (0,minv.dataPosition) and\n// (maxv.dataPosition,end) off\nasync function unzipChunkSlice(inputData: Buffer, chunk: Chunk) {\n  try {\n    let strm\n    const { minv, maxv } = chunk\n    let cpos = minv.blockPosition\n    let dpos = minv.dataPosition\n    const chunks = []\n    const cpositions = []\n    const dpositions = []\n\n    let totalSize = 0\n    let i = 0\n    do {\n      const remainingInput = inputData.subarray(cpos - minv.blockPosition)\n      const inflator = new Inflate()\n      // @ts-ignore\n      ;({ strm } = inflator)\n      inflator.push(remainingInput, Z_SYNC_FLUSH)\n      if (inflator.err) {\n        throw new Error(inflator.msg)\n      }\n\n      const buffer = inflator.result\n      chunks.push(buffer as Uint8Array)\n      let len = buffer.length\n\n      cpositions.push(cpos)\n      dpositions.push(dpos)\n      if (chunks.length === 1 && minv.dataPosition) {\n        // this is the first chunk, trim it\n        chunks[0] = chunks[0].subarray(minv.dataPosition)\n        len = chunks[0].length\n      }\n      const origCpos = cpos\n      cpos += strm.next_in\n      dpos += len\n\n      if (origCpos >= maxv.blockPosition) {\n        // this is the last chunk, trim it and stop decompressing\n        // note if it is the same block is minv it subtracts that already\n        // trimmed part of the slice length\n\n        chunks[i] = chunks[i].subarray(\n          0,\n          maxv.blockPosition === minv.blockPosition\n            ? maxv.dataPosition - minv.dataPosition + 1\n            : maxv.dataPosition + 1,\n        )\n\n        cpositions.push(cpos)\n        dpositions.push(dpos)\n        totalSize += chunks[i].length\n        break\n      }\n      totalSize += chunks[i].length\n      i++\n    } while (strm.avail_in)\n\n    const result = new Uint8Array(totalSize)\n    for (let i = 0, offset = 0; i < chunks.length; i++) {\n      result.set(chunks[i], offset)\n      offset += chunks[i].length\n    }\n    const buffer = Buffer.from(result)\n\n    return { buffer, cpositions, dpositions }\n  } catch (e) {\n    //cleanup error message\n    if (`${e}`.match(/incorrect header check/)) {\n      throw new Error(\n        'problem decompressing block: incorrect gzip header check',\n      )\n    }\n    throw e\n  }\n}\n\nfunction nodeUnzip() {\n  throw new Error('nodeUnzip not implemented.')\n}\n\nexport { unzip, unzipChunk, unzipChunkSlice, unzip as pakoUnzip, nodeUnzip }\n","import Long from 'long'\nimport { Buffer } from 'buffer'\nimport { LocalFile, GenericFilehandle } from 'generic-filehandle'\n\n// const COMPRESSED_POSITION = 0\nconst UNCOMPRESSED_POSITION = 1\n\nexport default class GziIndex {\n  filehandle: GenericFilehandle\n\n  index?: any\n\n  constructor({\n    filehandle,\n    path,\n  }: {\n    filehandle?: GenericFilehandle\n    path?: string\n  }) {\n    if (filehandle) {\n      this.filehandle = filehandle\n    } else if (path) {\n      this.filehandle = new LocalFile(path)\n    } else {\n      throw new TypeError('either filehandle or path must be defined')\n    }\n  }\n\n  _readLongWithOverflow(buf: Buffer, offset = 0, unsigned = true) {\n    //@ts-ignore\n    const long = Long.fromBytesLE(buf.slice(offset, offset + 8), unsigned)\n    if (\n      long.greaterThan(Number.MAX_SAFE_INTEGER) ||\n      long.lessThan(Number.MIN_SAFE_INTEGER)\n    ) {\n      throw new TypeError('integer overflow')\n    }\n\n    return long.toNumber()\n  }\n\n  _getIndex() {\n    if (!this.index) {\n      this.index = this._readIndex()\n    }\n    return this.index\n  }\n\n  async _readIndex() {\n    let buf = Buffer.allocUnsafe(8)\n    await this.filehandle.read(buf, 0, 8, 0)\n    const numEntries = this._readLongWithOverflow(buf, 0, true)\n    if (!numEntries) {\n      return [[0, 0]]\n    }\n\n    const entries = new Array(numEntries + 1)\n    entries[0] = [0, 0]\n\n    // TODO rewrite this to make an index-index that stays in memory\n    const bufSize = 8 * 2 * numEntries\n    if (bufSize > Number.MAX_SAFE_INTEGER) {\n      throw new TypeError('integer overflow')\n    }\n    buf = Buffer.allocUnsafe(bufSize)\n    await this.filehandle.read(buf, 0, bufSize, 8)\n    for (let entryNumber = 0; entryNumber < numEntries; entryNumber += 1) {\n      const compressedPosition = this._readLongWithOverflow(\n        buf,\n        entryNumber * 16,\n      )\n      const uncompressedPosition = this._readLongWithOverflow(\n        buf,\n        entryNumber * 16 + 8,\n      )\n      entries[entryNumber + 1] = [compressedPosition, uncompressedPosition]\n    }\n\n    return entries\n  }\n\n  async getLastBlock() {\n    const entries = await this._getIndex()\n    if (!entries.length) {\n      return undefined\n    }\n    return entries[entries.length - 1]\n  }\n\n  async getRelevantBlocksForRead(length: number, position: number) {\n    const endPosition = position + length\n    if (length === 0) {\n      return []\n    }\n    const entries = await this._getIndex()\n    const relevant = []\n\n    // binary search to find the block that the\n    // read starts in and extend forward from that\n    const compare = (entry: any, nextEntry: any) => {\n      const uncompressedPosition = entry[UNCOMPRESSED_POSITION]\n      const nextUncompressedPosition = nextEntry\n        ? nextEntry[UNCOMPRESSED_POSITION]\n        : Infinity\n      // block overlaps read start\n      if (\n        uncompressedPosition <= position &&\n        nextUncompressedPosition > position\n      ) {\n        return 0\n        // block is before read start\n      }\n      if (uncompressedPosition < position) {\n        return -1\n      }\n      // block is after read start\n      return 1\n    }\n\n    let lowerBound = 0\n    let upperBound = entries.length - 1\n    let searchPosition = Math.floor(entries.length / 2)\n\n    let comparison = compare(\n      entries[searchPosition],\n      entries[searchPosition + 1],\n    )\n    while (comparison !== 0) {\n      if (comparison > 0) {\n        upperBound = searchPosition - 1\n      } else if (comparison < 0) {\n        lowerBound = searchPosition + 1\n      }\n      searchPosition = Math.ceil((upperBound - lowerBound) / 2) + lowerBound\n      comparison = compare(entries[searchPosition], entries[searchPosition + 1])\n    }\n\n    // here's where we read forward\n    relevant.push(entries[searchPosition])\n    let i = searchPosition + 1\n    for (; i < entries.length; i += 1) {\n      relevant.push(entries[i])\n      if (entries[i][UNCOMPRESSED_POSITION] >= endPosition) {\n        break\n      }\n    }\n    if (relevant[relevant.length - 1][UNCOMPRESSED_POSITION] < endPosition) {\n      relevant.push([])\n    }\n    return relevant\n  }\n}\n","import { Buffer } from 'buffer'\nimport { LocalFile, GenericFilehandle } from 'generic-filehandle'\n\n// locals\nimport { unzip } from './unzip'\nimport GziIndex from './gziIndex'\n\nexport default class BgzFilehandle {\n  filehandle: GenericFilehandle\n  gzi: GziIndex\n\n  constructor({\n    filehandle,\n    path,\n    gziFilehandle,\n    gziPath,\n  }: {\n    filehandle?: GenericFilehandle\n    path?: string\n    gziFilehandle?: GenericFilehandle\n    gziPath?: string\n  }) {\n    if (filehandle) {\n      this.filehandle = filehandle\n    } else if (path) {\n      this.filehandle = new LocalFile(path)\n    } else {\n      throw new TypeError('either filehandle or path must be defined')\n    }\n\n    if (!gziFilehandle && !gziPath && !path) {\n      throw new TypeError('either gziFilehandle or gziPath must be defined')\n    }\n\n    this.gzi = new GziIndex({\n      filehandle: gziFilehandle,\n      path: !gziFilehandle && !gziPath && path ? gziPath : `${path}.gzi`,\n    })\n  }\n\n  async stat() {\n    const compressedStat = await this.filehandle.stat()\n    return Object.assign(compressedStat, {\n      size: await this.getUncompressedFileSize(),\n      blocks: undefined,\n      blksize: undefined,\n    })\n  }\n\n  async getUncompressedFileSize() {\n    // read the last block's ISIZE (see gzip RFC),\n    // and add it to its uncompressedPosition\n    const [, uncompressedPosition] = await this.gzi.getLastBlock()\n\n    const { size } = await this.filehandle.stat()\n\n    const buf = Buffer.allocUnsafe(4)\n    // note: there should be a 28-byte EOF marker (an empty block) at\n    // the end of the file, so we skip backward past that\n    const { bytesRead } = await this.filehandle.read(buf, 0, 4, size - 28 - 4)\n    if (bytesRead !== 4) {\n      throw new Error('read error')\n    }\n    const lastBlockUncompressedSize = buf.readUInt32LE(0)\n    return uncompressedPosition + lastBlockUncompressedSize\n  }\n\n  async _readAndUncompressBlock(\n    blockBuffer: Buffer,\n    [compressedPosition]: [number],\n    [nextCompressedPosition]: [number],\n  ) {\n    let next = nextCompressedPosition\n    if (!next) {\n      next = (await this.filehandle.stat()).size\n    }\n\n    // read the compressed data into the block buffer\n    const blockCompressedLength = next - compressedPosition\n\n    await this.filehandle.read(\n      blockBuffer,\n      0,\n      blockCompressedLength,\n      compressedPosition,\n    )\n\n    // uncompress it\n    const unzippedBuffer = await unzip(\n      blockBuffer.slice(0, blockCompressedLength),\n    )\n\n    return unzippedBuffer as Buffer\n  }\n\n  async read(buf: Buffer, offset: number, length: number, position: number) {\n    // get the block positions for this read\n    const blockPositions = await this.gzi.getRelevantBlocksForRead(\n      length,\n      position,\n    )\n    const blockBuffer = Buffer.allocUnsafe(32768 * 2)\n    // uncompress the blocks and read from them one at a time to keep memory usage down\n    let destinationOffset = offset\n    let bytesRead = 0\n    for (\n      let blockNum = 0;\n      blockNum < blockPositions.length - 1;\n      blockNum += 1\n    ) {\n      // eslint-disable-next-line no-await-in-loop\n      const uncompressedBuffer = await this._readAndUncompressBlock(\n        blockBuffer,\n        blockPositions[blockNum],\n        blockPositions[blockNum + 1],\n      )\n      const [, uncompressedPosition] = blockPositions[blockNum]\n      const sourceOffset =\n        uncompressedPosition >= position ? 0 : position - uncompressedPosition\n      const sourceEnd =\n        Math.min(\n          position + length,\n          uncompressedPosition + uncompressedBuffer.length,\n        ) - uncompressedPosition\n      if (sourceOffset >= 0 && sourceOffset < uncompressedBuffer.length) {\n        uncompressedBuffer.copy(buf, destinationOffset, sourceOffset, sourceEnd)\n        destinationOffset += sourceEnd - sourceOffset\n        bytesRead += sourceEnd - sourceOffset\n      }\n    }\n\n    return { bytesRead, buffer: buf }\n  }\n}\n","import { LocalFile, GenericFilehandle } from 'generic-filehandle'\nimport { Buffer } from 'buffer'\n\ninterface BaseOpts {\n  signal?: AbortSignal\n}\n\ninterface IndexEntry {\n  offset: number\n  lineBytes: number\n  lineLength: number\n  length: number\n}\n\nfunction _faiOffset(idx: IndexEntry, pos: number) {\n  return (\n    idx.offset +\n    idx.lineBytes * Math.floor(pos / idx.lineLength) +\n    (pos % idx.lineLength)\n  )\n}\n\nasync function readFAI(fai: GenericFilehandle, opts?: BaseOpts) {\n  const text = await fai.readFile(opts)\n  if (!text?.length) {\n    throw new Error('No data read from FASTA index (FAI) file')\n  }\n\n  let idCounter = 0\n  let currSeq: { name: string; id: number } | undefined\n  const data = text\n    .toString('utf8')\n    .split(/\\r?\\n/)\n    .filter(line => /\\S/.test(line))\n    .map(line => line.split('\\t'))\n    .filter(row => row[0] !== '')\n    .map(row => {\n      if (!currSeq || currSeq.name !== row[0]) {\n        currSeq = { name: row[0], id: idCounter }\n        idCounter += 1\n      }\n\n      return {\n        id: currSeq.id,\n        name: row[0],\n        length: +row[1],\n        start: 0,\n        end: +row[1],\n        offset: +row[2],\n        lineLength: +row[3],\n        lineBytes: +row[4],\n      }\n    })\n\n  return {\n    name: Object.fromEntries(data.map(entry => [entry.name, entry])),\n    id: Object.fromEntries(data.map(entry => [entry.id, entry])),\n  }\n}\n\nexport default class IndexedFasta {\n  fasta: GenericFilehandle\n  fai: GenericFilehandle\n  indexes?: ReturnType<typeof readFAI>\n\n  constructor({\n    fasta,\n    fai,\n    path,\n    faiPath,\n  }: {\n    fasta?: GenericFilehandle\n    fai?: GenericFilehandle\n    path?: string\n    faiPath?: string\n  }) {\n    if (fasta) {\n      this.fasta = fasta\n    } else if (path) {\n      this.fasta = new LocalFile(path)\n    } else {\n      throw new Error('Need to pass filehandle for fasta or path to localfile')\n    }\n\n    if (fai) {\n      this.fai = fai\n    } else if (faiPath) {\n      this.fai = new LocalFile(faiPath)\n    } else if (path) {\n      this.fai = new LocalFile(`${path}.fai`)\n    } else {\n      throw new Error('Need to pass filehandle for  or path to localfile')\n    }\n  }\n\n  async _getIndexes(opts?: BaseOpts) {\n    if (!this.indexes) {\n      this.indexes = readFAI(this.fai, opts)\n    }\n    return this.indexes\n  }\n\n  /**\n   * @returns {array[string]} array of string sequence\n   * names that are present in the index, in which the\n   * array index indicates the sequence ID, and the value\n   * is the sequence name\n   */\n  async getSequenceNames(opts?: BaseOpts) {\n    return Object.keys((await this._getIndexes(opts)).name)\n  }\n\n  /**\n   * @returns {array[string]} array of string sequence\n   * names that are present in the index, in which the\n   * array index indicates the sequence ID, and the value\n   * is the sequence name\n   */\n  async getSequenceSizes(opts?: BaseOpts) {\n    const returnObject = {} as Record<string, number>\n    const idx = await this._getIndexes(opts)\n    for (const val of Object.values(idx.id)) {\n      returnObject[val.name] = val.length\n    }\n    return returnObject\n  }\n\n  /**\n   * @returns {array[string]} array of string sequence\n   * names that are present in the index, in which the\n   * array index indicates the sequence ID, and the value\n   * is the sequence name\n   */\n  async getSequenceSize(seqName: string, opts?: BaseOpts) {\n    const idx = await this._getIndexes(opts)\n    return idx.name[seqName]?.length\n  }\n\n  /**\n   *\n   * @param {string} name\n   * @returns {Promise[boolean]} true if the file contains the given reference sequence name\n   */\n  async hasReferenceSequence(name: string, opts?: BaseOpts) {\n    return !!(await this._getIndexes(opts)).name[name]\n  }\n\n  /**\n   *\n   * @param {number} seqId\n   * @param {number} min\n   * @param {number} max\n   */\n  async getResiduesById(\n    seqId: number,\n    min: number,\n    max: number,\n    opts?: BaseOpts,\n  ) {\n    const indexEntry = (await this._getIndexes(opts)).id[seqId]\n    if (!indexEntry) {\n      return undefined\n    }\n    return this._fetchFromIndexEntry(indexEntry, min, max, opts)\n  }\n\n  /**\n   * @param {string} seqName\n   * @param {number} min\n   * @param {number} max\n   */\n  async getResiduesByName(\n    seqName: string,\n    min: number,\n    max: number,\n    opts?: BaseOpts,\n  ) {\n    const indexEntry = (await this._getIndexes(opts)).name[seqName]\n    if (!indexEntry) {\n      return undefined\n    }\n\n    return this._fetchFromIndexEntry(indexEntry, min, max, opts)\n  }\n\n  //alias for getResiduesByName\n  async getSequence(\n    seqName: string,\n    min: number,\n    max: number,\n    opts?: BaseOpts,\n  ) {\n    return this.getResiduesByName(seqName, min, max, opts)\n  }\n\n  async _fetchFromIndexEntry(\n    indexEntry: IndexEntry,\n    min = 0,\n    max: number,\n    opts?: BaseOpts,\n  ) {\n    let end = max\n    if (min < 0) {\n      throw new TypeError('regionStart cannot be less than 0')\n    }\n    if (end === undefined || end > indexEntry.length) {\n      end = indexEntry.length\n    }\n    if (min >= end) {\n      return ''\n    }\n\n    const position = _faiOffset(indexEntry, min)\n    const readlen = _faiOffset(indexEntry, end) - position\n\n    const residues = Buffer.allocUnsafe(readlen)\n    await this.fasta.read(residues, 0, readlen, position, opts)\n    return residues.toString('utf8').replace(/\\s+/g, '')\n  }\n}\n","import { BgzfFilehandle } from '@gmod/bgzf-filehandle'\nimport { GenericFilehandle } from 'generic-filehandle'\nimport IndexedFasta from './indexedFasta'\n\nexport default class BgzipIndexedFasta extends IndexedFasta {\n  constructor({\n    fasta,\n    path,\n    fai,\n    faiPath,\n    gzi,\n    gziPath,\n  }: {\n    fasta?: GenericFilehandle\n    path?: string\n    fai?: GenericFilehandle\n    faiPath?: string\n    gzi?: GenericFilehandle\n    gziPath?: string\n  }) {\n    super({ fasta, path, fai, faiPath })\n    if (fasta && gzi) {\n      // @ts-expect-error\n      this.fasta = new BgzfFilehandle({\n        filehandle: fasta,\n        gziFilehandle: gzi,\n      })\n    } else if (path && gziPath) {\n      // @ts-expect-error\n      this.fasta = new BgzfFilehandle({ path, gziPath })\n    }\n  }\n}\n"],"names":["NullSignal","AggregateAbortController","signals","Set","abortController","AbortController","addSignal","signal","this","aborted","Error","add","handleAborted","addEventListener","delete","size","abort","AggregateStatusReporter","callbacks","addCallback","callback","currentMessage","message","elt","AbortablePromiseCache","constructor","fill","cache","TypeError","get","set","fillCallback","isAbortException","exception","name","code","evict","key","entry","data","statusCallback","aborter","statusReporter","newEntry","promise","settled","then","catch","error","console","checkSinglePromise","checkForSingleAbort","Object","assign","result","has","AbortSignal","cacheEntry","cachedEntry","clear","keyIter","keys","deleteCount","next","done","value","async","unzip","inputData","strm","pos","i","chunks","inflator","totalSize","remainingInput","subarray","Inflate","push","Z_SYNC_FLUSH","err","msg","next_in","length","avail_in","Uint8Array","offset","from","e","match","unzipChunkSlice","chunk","minv","maxv","cpos","blockPosition","dpos","dataPosition","cpositions","dpositions","buffer","len","origCpos","GziIndex","filehandle","path","_readLongWithOverflow","buf","unsigned","long","slice","greaterThan","Number","MAX_SAFE_INTEGER","lessThan","MIN_SAFE_INTEGER","toNumber","_getIndex","index","_readIndex","allocUnsafe","read","numEntries","entries","Array","bufSize","entryNumber","compressedPosition","uncompressedPosition","getLastBlock","getRelevantBlocksForRead","position","endPosition","relevant","compare","nextEntry","nextUncompressedPosition","Infinity","lowerBound","upperBound","searchPosition","Math","floor","comparison","ceil","BgzFilehandle","gziFilehandle","gziPath","gzi","stat","compressedStat","getUncompressedFileSize","blocks","undefined","blksize","bytesRead","readUInt32LE","_readAndUncompressBlock","blockBuffer","nextCompressedPosition","blockCompressedLength","blockPositions","destinationOffset","blockNum","uncompressedBuffer","sourceOffset","sourceEnd","min","copy","_faiOffset","idx","lineBytes","lineLength","IndexedFasta","fasta","fai","faiPath","_getIndexes","opts","indexes","text","readFile","currSeq","idCounter","toString","split","filter","line","test","map","row","id","start","end","fromEntries","readFAI","getSequenceNames","getSequenceSizes","returnObject","val","values","getSequenceSize","seqName","hasReferenceSequence","getResiduesById","seqId","max","indexEntry","_fetchFromIndexEntry","getResiduesByName","getSequence","readlen","residues","replace","BgzipIndexedFasta","super"],"sourceRoot":""}